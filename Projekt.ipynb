{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titel\n",
    "## Mauro Schegg\n",
    "## 3.3.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zielsetzung und Vorgehensweise:\n",
    "Es soll ein Modell erstellt werden welches Zeitreihenprognosen erstellt. Es soll ein RNN mit LSTM angewendet werden um diese Prognosen zu erstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Daten\n",
    "dfp = pd.read_csv('Daten/train.csv')\n",
    "dfwf1 = pd.read_csv('Daten/windforecasts_wf1.csv')\n",
    "dfwf2 = pd.read_csv('Daten/windforecasts_wf2.csv')\n",
    "dfwf3 = pd.read_csv('Daten/windforecasts_wf3.csv')\n",
    "dfwf4 = pd.read_csv('Daten/windforecasts_wf4.csv')\n",
    "dfwf5 = pd.read_csv('Daten/windforecasts_wf5.csv')\n",
    "dfwf6 = pd.read_csv('Daten/windforecasts_wf6.csv')\n",
    "dfwf7 = pd.read_csv('Daten/windforecasts_wf7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode zur Umwandlung in datetime für alle DataFrames\n",
    "def convert_to_datetime(dfs):\n",
    "\tfor df in dfs:\n",
    "\t\tdf['date'] = pd.to_datetime(df['date'], format='%Y%m%d%H')\n",
    "\treturn dfs\n",
    "\n",
    "# Liste der DataFrames\n",
    "dfs = [dfp,dfwf1, dfwf2, dfwf3, dfwf4, dfwf5, dfwf6, dfwf7]\n",
    "\n",
    "# Aufruf der Funktion\n",
    "dfs = convert_to_datetime(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die ersten 5 Zeilen der Leistung der Windparks\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die ersten 5 Zeilen der Windvorhersagen für Windpark 1\n",
    "dfwf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erkenntnisse:**\n",
    "- Es werden alle 12h die Winddaten für 48h vorraus gesagt, dies bedeutet, dass es 4 Vorhersagen für die meisten Zeitpunkte gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionen des Datensatzes\n",
    "print(dfp.shape)\n",
    "print(dfwf1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Das DataFrame für die Leistungen der Windpark hat 8 Spalten eine davon ist die Zeit, das heisst es hat 7 Windparks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse der Datentypen\n",
    "dfp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfen des leistungs DataFrames auf fehlende Werte\n",
    "print(dfp.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfen des leistungs DataFrames auf fehlende Werte\n",
    "print(dfwf1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlende Werte durch Mittelwert ersetzen.\n",
    "# dfs = [dfwf1, dfwf2, dfwf3, dfwf4, dfwf5, dfwf6, dfwf7]\n",
    "# for i in range(len(dfs)):\n",
    "    # dfs[i].fillna(dfs[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwf1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es hat keine fehlenden Daten im leistungs Datensatz.\n",
    "- Es hat $11160$ fehlende Daten bei jeder der Spalten $u,v,w_s,w_d$. Dies sind etwa $11\\%$ der gesamten Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deskriptive Statistik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.describe()\n",
    "sns.boxplot(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jedes dieser Boxplots beschreibt die normalisierten Daten für die Leistung eines Windparks.\n",
    "- Windpark 1 scheint Aussreisser zu haben welche betrachtet werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umwandlung des leistungs Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandlung dfp\n",
    "dfp = dfp.rename(columns={\"wp1\":\"wf1\",\"wp2\":\"wf2\",\"wp3\":\"wf3\",\"wp4\":\"wf4\",\"wp5\":\"wf5\",\"wp6\":\"wf6\",\"wp7\":\"wf7\"})\n",
    "# Zusammenführen der Leistungen in eine Spalte\n",
    "dfp = dfp.melt(id_vars=[\"date\"], value_name=\"wpower\",var_name=\"wfarm\",ignore_index=True)\n",
    "dfp[\"wfarm\"] = dfp[\"wfarm\"].astype(\"string\")\n",
    "dfp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prognosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(dfwf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zeitabschnitte**\n",
    "- hors beschreibt die wie vielte Stunde der Vorhersage das es ist. Dies sind Werte von 1-48.\n",
    "\n",
    "**Kartesische Koordinaten:**\n",
    "- u beschreibt die Windgeschwindigkeit in Ost-West-Richtung\n",
    "- v beschreibt die Windgeschwindigkeit in Nord-Süd-Richtung\n",
    "\n",
    "**Polar Koordinaten:***\n",
    "- ws = $\\sqrt{u^2+v^2}$ Betrag des Geschwindigkeitsvektors\n",
    "- wf beschreibt die Richtung des Windes in Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der DataFrames und zugehörige Keys\n",
    "dfs = [dfwf1, dfwf2, dfwf3, dfwf4, dfwf5, dfwf6, dfwf7]\n",
    "keys = [\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"]\n",
    "\n",
    "# Neue DataFrames mit 'wfarm'-Spalte erstellen\n",
    "dfs = [df.assign(wfarm=key) for df, key in zip(dfs, keys)]\n",
    "\n",
    "# DataFrames zusammenfügen\n",
    "dfw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Spalten umsortieren, damit 'wfarm' ganz links steht\n",
    "cols = [\"wfarm\"] + [col for col in dfw.columns if col != \"wfarm\"]\n",
    "dfw = dfw[cols]\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "dfw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw[\"forecast\"] = pd.to_datetime(dfw[\"date\"],format=\"%Y%m%d%H\")\n",
    "dfw[\"delta_hours\"] = pd.to_timedelta(dfw[\"hors\"],unit=\"hours\")\n",
    "dfw[\"forecast_date\"] = dfw[\"forecast\"]+dfw[\"delta_hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding periodischer Merkmale\n",
    "# Richtung\n",
    "dfw[\"wd_sin\"] = np.sin(2*np.pi*dfw[\"wd\"]/360)\n",
    "dfw[\"wd_cos\"] = np.cos(2**np.pi*dfw[\"wd\"]/360)\n",
    "# Monat\n",
    "dfw[\"month\"] = dfw.forecast_date.dt.month\n",
    "dfw[\"month_sin\"] = np.sin(2**np.pi*dfw[\"month\"]/12)\n",
    "dfw[\"month_cos\"] = np.cos(2**np.pi*dfw[\"month\"]/12)\n",
    "# Tag\n",
    "dfw[\"day\"] = dfw.forecast_date.dt.day\n",
    "dfw[\"day_sin\"] = np.sin(2**np.pi*dfw[\"day\"]/31)\n",
    "dfw[\"day_cos\"] = np.cos(2**np.pi*dfw[\"day\"]/31)\n",
    "# Stunde\n",
    "dfw[\"hour\"] = dfw.forecast_date.dt.hour\n",
    "dfw[\"hour_sin\"] = np.sin(2**np.pi*dfw[\"hour\"]/24)\n",
    "dfw[\"hour_cos\"] = np.cos(2**np.pi*dfw[\"hour\"]/24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gleitender Mittelwert\n",
    "hgm = 6 \n",
    "dfw[\"u_gm\"] = dfw.u.rolling(hgm).mean()\n",
    "dfw[\"v_gm\"] = dfw.v.rolling(hgm).mean()\n",
    "dfw[\"ws_gm\"] = dfw.ws.rolling(hgm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfw[\"forecast\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung der beiden Dataframes\n",
    "df = dfp.merge(dfw,how=\"outer\",left_on=[\"date\",\"wfarm\"],right_on=[\"forecast_date\",\"wfarm\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"date_x\"]\n",
    "del df[\"date_y\"]\n",
    "del df[\"hors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['wpower', 'u', 'v', 'ws', 'wd','wd_sin', 'wd_cos','month_sin', 'month_cos','day_sin',\n",
    "       'day_cos','hour_sin', 'hour_cos', 'u_gm', 'v_gm', 'ws_gm']].corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr,annot=True,cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df.sample(n=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tree um die wichtigsten Features zu finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = df.sample(2000)\n",
    "dfTest.dropna(axis=0,inplace=True)\n",
    "dfTest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfTest[['u', 'v', 'ws', 'wd','wd_sin', 'wd_cos','month_sin', 'month_cos',\n",
    "       'day_cos','hour_sin', 'hour_cos', 'u_gm']]\n",
    "y = dfTest[\"wpower\"]\n",
    "\n",
    "# Entscheidungsbaum-Regression mit max_depth\n",
    "forest = RandomForestRegressor(n_estimators=1000,max_depth=250,random_state=None)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = forest.feature_importances_\n",
    "importance_df = pd.DataFrame({\"Features\":X.columns,\"Importance\":feature_importance})\n",
    "importance_df.sort_values(by=\"Importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = importance_df.sort_values(by=\"Importance\",ascending=False).head(6)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(top_features[\"Features\"],top_features[\"Importance\"])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance der einzelnen Komponenten\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wie zu erwarten ist die Windgeschwindigkeit einer der wichtigsten Faktoren für die Leistung eines Windkraftwerks.\n",
    "- $ws_{gm}$, $v_{gm}$, $day_{sin}$ wurden entfernt das sie sonst doppelt representiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bewertungskriterium:\n",
    "- Es wird danach bewertet wie gut das Modell die Leistung vorraus sagt.\n",
    "- Dazu wird die **Root Mean Squared Error (RMSE)** Loss Funktion verwendet, da sie grosse Fehler bestraft und das Ergebnis in der Einheit der Leistung interpretiert werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bewertungsmethode\n",
    "- Holdout-Methode kann verwendet werden, da der Datensatz sehr gross ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline-Modell:\n",
    "- Für das Baseline-Modell wird ein LTSM verwendet welches speziell für Zeitabhängige Daten entwickelt wurde. RNN haben das *Vanishing Gradient* Problem, dies hat LTSM nicht. \n",
    "- Das *Vanishing Gradient* Problem ist wenn die Gradienten während des Backpropagation-Trainings immer kleiner werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameter für die LSTM-Eingabe\n",
    "TIME_STEPS = 48  # Anzahl vergangener Stunden für die Vorhersage der nächsten Stunde\n",
    "\n",
    "# Modell-Parameter\n",
    "input_size = 6  # Anzahl der Features\n",
    "output_size = 1\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "hidden_sizes = [50, 100, 150]\n",
    "num_layers_list = [2, 3, 4]  # Verschiedene Anzahl der LSTM-Schichten\n",
    "\n",
    "# LSTM Modell definieren\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Nur letztes Zeitfenster nutzen\n",
    "\n",
    "# Modell, Verlustfunktion und RMSE-Definition\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    mse = criterion(y_pred, y_true)\n",
    "    rmse = torch.sqrt(mse)  # RMSE Loss Funktion\n",
    "    return rmse\n",
    "\n",
    "# Liste der Resultate \n",
    "results = []\n",
    "\n",
    "# Iteration über alle Windparks und Kombinationen der Hyperparameter\n",
    "for wf_id in dfTest[\"wfarm\"].unique():\n",
    "    print(f\"Training für Windpark {wf_id}...\")\n",
    "\n",
    "    # Filtern der Daten für den Windpark\n",
    "    df_wf = dfTest[dfTest[\"wfarm\"] == \"wf1\"]\n",
    "\n",
    "    # Features und Zielvariable\n",
    "    X = torch.tensor(df_wf[[\"ws\", \"u_gm\", \"v\", \"day_cos\", \"hour_cos\", \"month_sin\"]].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df_wf[\"wpower\"].values, dtype=torch.float32).view(-1, 1)  # 2D-Shape für LSTM\n",
    "\n",
    "    # Min-Max-Normalisierung mit PyTorch (statt Sklearn)\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_scaled = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Funktion zur Erstellung von Sequenzen für das LSTM\n",
    "    def create_sequences(X, y, time_steps):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            Xs.append(X[i:i + time_steps])  # 48 Werte als Eingabe\n",
    "            ys.append(y[i + time_steps])    # Der nächste Wert als Ziel\n",
    "        return torch.stack(Xs), torch.stack(ys)\n",
    "\n",
    "    # Erstelle die Sequenzen\n",
    "    X_seq, y_seq = create_sequences(X, y_scaled, TIME_STEPS)\n",
    "\n",
    "    # Holdout-Methode: Train (70%), Validation (10%), Test (20%) Split\n",
    "    train_split = int(0.7 * len(X_seq))\n",
    "    val_split = int(0.8 * len(X_seq))  # 70% + 10% = 80%\n",
    "\n",
    "    X_train, X_val, X_test = X_seq[:train_split], X_seq[train_split:val_split], X_seq[val_split:]\n",
    "    y_train, y_val, y_test = y_seq[:train_split], y_seq[train_split:val_split], y_seq[val_split:]\n",
    "\n",
    "    # Prüfe die Shapes\n",
    "    print(f\"X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\n",
    "    print(f\"X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}\")\n",
    "    print(f\"X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")\n",
    "\n",
    "    # Testen der verschiedenen Lernraten, versteckten Schichtgrößen und LSTM-Schichten\n",
    "    for lr in learning_rates:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for num_layers in num_layers_list:\n",
    "                print(f\"Training mit Lernrate: {lr}, Hidden Size: {hidden_size}, num_layers: {num_layers}...\")\n",
    "\n",
    "                # Modell neu erstellen mit den aktuellen Hyperparametern\n",
    "                model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                # Training\n",
    "                EPOCHS = 50\n",
    "                for epoch in range(EPOCHS):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(X_train)\n",
    "                    loss = compute_rmse(y_pred, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Validierung\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_pred = model(X_val)\n",
    "                        val_loss = compute_rmse(val_pred, y_val)\n",
    "\n",
    "                    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "                # Modell testen\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_test_pred = model(X_test)\n",
    "\n",
    "                # Test-Fehler berechnen\n",
    "                test_loss = compute_rmse(y_test_pred, y_test)\n",
    "                print(f\"Test Loss für Windpark {wf_id} mit Lernrate {lr}, Hidden Size {hidden_size}, und num_layers {num_layers}: {test_loss.item():.4f}\")\n",
    "\n",
    "                # Ergebnisse speichern\n",
    "                results.append({\n",
    "                    \"wf_id\": wf_id,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"hidden_size\": hidden_size,\n",
    "                    \"num_layers\": num_layers,\n",
    "                    \"test_loss\": test_loss.item()\n",
    "                })\n",
    "\n",
    "# Speichern der Ergebnisse in einem DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ausgabe der Resultate\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_sort = results_df.sort_values(by='test_loss',ascending=True)\n",
    "results_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameter für die LSTM-Eingabe\n",
    "TIME_STEPS = 48  # Anzahl vergangener Stunden für die Vorhersage der nächsten Stunde\n",
    "\n",
    "# Modell-Parameter\n",
    "input_size = 6  # Anzahl der Features\n",
    "hidden_size = 80\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "hidden_sizes = [50, 100, 150]\n",
    "\n",
    "# LSTM Modell definieren\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Nur letztes Zeitfenster nutzen\n",
    "    \n",
    "# Modell, Verlustfunktion & \n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    mse = criterion(y_pred, y_true)\n",
    "    rmse = torch.sqrt(mse) # RMSE Loss Funktion\n",
    "    return rmse\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iteration über alle Windparks\n",
    "for wf_id in dfTest[\"wfarm\"].unique():\n",
    "    print(f\"Training für Windpark {wf_id}...\")\n",
    "    \n",
    "    # Filtern der Daten für den Windpark\n",
    "    df_wf = dfTest[dfTest[\"wfarm\"] == \"wf1\"]\n",
    "    \n",
    "    # Features und Zielvariable\n",
    "    X = torch.tensor(df_wf[[\"ws\", \"u_gm\", \"v\", \"day_cos\", \"hour_cos\", \"month_sin\"]].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df_wf[\"wpower\"].values, dtype=torch.float32).view(-1, 1)  # 2D-Shape für LSTM\n",
    "\n",
    "    # Min-Max-Normalisierung mit PyTorch (statt Sklearn)\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_scaled = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Funktion zur Erstellung von Sequenzen für das LSTM\n",
    "    def create_sequences(X, y, time_steps):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            Xs.append(X[i:i + time_steps])  # 48 Werte als Eingabe\n",
    "            ys.append(y[i + time_steps])    # Der nächste Wert als Ziel\n",
    "        return torch.stack(Xs), torch.stack(ys)\n",
    "\n",
    "    # Erstelle die Sequenzen\n",
    "    X_seq, y_seq = create_sequences(X, y_scaled, TIME_STEPS)\n",
    "\n",
    "\t\t\t\t# Holdout-MEthode\n",
    "    # Train (70%), Validation (10%), Test (20%) Split in PyTorch\n",
    "    train_split = int(0.7 * len(X_seq))\n",
    "    val_split = int(0.8 * len(X_seq))  # 70% + 10% = 80%\n",
    "\n",
    "    X_train, X_val, X_test = X_seq[:train_split], X_seq[train_split:val_split], X_seq[val_split:]\n",
    "    y_train, y_val, y_test = y_seq[:train_split], y_seq[train_split:val_split], y_seq[val_split:]\n",
    "\n",
    "    # Prüfe die Shapes\n",
    "    print(f\"X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\n",
    "    print(f\"X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}\")\n",
    "    print(f\"X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")\n",
    "\n",
    "    # Training\n",
    "    EPOCHS = 50\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = compute_rmse(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validierung\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = compute_rmse(val_pred, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Modell testen\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_test)\n",
    "\n",
    "    # Test-Fehler berechnen sollte niedrig sein\n",
    "    test_loss = compute_rmse(y_test_pred, y_test)\n",
    "    print(f\"Test Loss für Windpark {wf_id}: {test_loss.item():.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvProjAnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
